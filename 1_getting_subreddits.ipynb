{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>AUTOMATING POST PULL REQUESTS</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './visuals/aita.png'>\n",
    "<a href= 'https://www.reddit.com/r/AmItheAsshole/'> Am I The Asshole </a>\n",
    "<img src = './visuals/jnmil.png'>\n",
    "<a href= 'https://www.reddit.com/r/JUSTNOMIL/'> Just NO Mother-In-Law </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my needed imports\n",
    "import pandas as pd   \n",
    "import requests \n",
    "import time   \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to automate pull requests from PushShift API for my 2 chosen subreddits\n",
    "\n",
    "The function will:\n",
    "- make a url request to pushshift using the subreddit of choice using parameters indicated\n",
    "- even if I put n_requests to 5000, the function will pull 100, wait for 30 seconds and run next 100\n",
    "    - based on timestamp on last pulled post, it will not repeat same posts\n",
    "    \n",
    "- A warning will be printed if :\n",
    "    - The status code is anything other than 200\n",
    "    - There is a connection error of unknown reason\n",
    "    - The pull request did not add anything to our new_list (nothing pulled / to pull)\n",
    "    \n",
    "- As the function is running it will inform mer:\n",
    "    - Which pull request it is on\n",
    "    - How many it pulled that request\n",
    "    - And what the total length of our big list is currently at\n",
    "    \n",
    "- When all requests have been pulled the posts will be:\n",
    "    - Saved to a dataframe, with columns: Author, title, subtext, subreddit\n",
    "    - The new dataframe will be saved to csv to be used later.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [] # empty posts list outside function so I can access later\n",
    "    \n",
    "def reddit_pull(subreddit, n_requests):\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/' # base url\n",
    "    last_obtained_time = round(time.time()) # will be updated with the timestamp of last aquired post\n",
    "    # initially I had it set to 0, but it wouldnt' get past the first if clause below. maybe because 0 is not a time?\n",
    "    # got time.time() from w3\n",
    "\n",
    "\n",
    "    pull = 1 # since we can only pull certain amounts in a 30 second timeframe, I will start with pull 1 and increment\n",
    "   \n",
    "    while len(posts) < n_requests: # while the length of list 'posts' is less than the number of requests I made\n",
    "        \n",
    "        try: # I'm going to do a try except to make sure I get notice if there is a connection error\n",
    "            \n",
    "            search_params = {\n",
    "                'subreddit': subreddit, # the subreddit of choice\n",
    "                'sort':'desc', # sort results in descending format\n",
    "                'size': n_requests,  # the size of the request total\n",
    "                'before': last_obtained_time-1,\n",
    "                'user_removed' : [True],\n",
    "                'mod_removed' : [True],\n",
    "                                 }\n",
    "            \n",
    "\n",
    "            r = requests.get(url, params = search_params) # make the request using the params set\n",
    "            data = r.json() # make the data into .json\n",
    "            new_posts = data['data'] # get the dictionary 'data'\n",
    "            \n",
    "            \n",
    "            if len(new_posts) == 0: # if the length of the new aquired posts is equal to 0\n",
    "                last_obtained_time = last_obtained_time # the last obtained time remains the same\n",
    "                # Do nothing else, don't extend the list, don't increase the pull\n",
    "                print('This pull request has not pulled any new posts')\n",
    "                \n",
    "            else:     \n",
    "                last_obtained_time = new_posts[-1]['created_utc'] #otherwise change the time to the last aquired post's time\n",
    "                posts.extend(new_posts) # extend adds all elements gathered from new_posts to the posts list, whereas .append will\n",
    "                                #take all elements gathered as 1 and add it to list\n",
    "                pulled = len(new_posts)        # which will tell me how many pulls it has made so far and how many posts in that pull\n",
    "                print(f'Pull Request {pull} has pulled {pulled} submissions')\n",
    "                print(f'There are now {len(posts)} total posts') # Lets me know which pull request it is and how many were pulled.\n",
    "                time.sleep(30) # sleep for 30 seconds before next pull\n",
    "                pull += 1 # adds 1 to pull counter to indicate it is the next pull\n",
    "                \n",
    "\n",
    "        \n",
    "               \n",
    "        # The except part, to throw up error if connection\n",
    "        except:\n",
    "            if r.status_code != 200: # if the status code is anything but 200\n",
    "                print(f'Connection Error Code: {r.status_code}')\n",
    "            else: # otherwise if the error is not status related:\n",
    "                print('Could not complete pull, Reason Unknown')\n",
    "                \n",
    "        ## Saving to DF and to csv for later use\n",
    "        df = pd.DataFrame(posts)[['author', 'title', 'selftext', 'subreddit']]\n",
    "        df.to_csv('./data/' + subreddit + '.csv')    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit_pull('AmItheAsshole', 9000) # 9000 because out of the original 5000 I pulled 1700 were removed\n",
    "                                    # even through I added filter in search params some still showed up\n",
    "                                    # 9000 will make sure I still have more than 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example of the print statements as function runs:\n",
    "- Pull Request 1 has pulled 100 submissions\n",
    "    - There are now 100 total posts\n",
    "- Pull Request 2 has pulled 100 submissions\n",
    "    - There are now 200 total posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df out of pulled data list\n",
    "# reddit_pull('justnomil', 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquired posts are now saved into a csv to be used in next notebook (Data Cleanup & EDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
